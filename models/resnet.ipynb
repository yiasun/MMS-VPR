{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ab349e27cbf68cf7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T13:14:37.189502Z",
     "start_time": "2025-05-21T13:14:37.173669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs     = 20\n",
    "lr             = 1e-3\n",
    "lr_bert        = 5e-5\n",
    "weight_decay   = 1e-2\n",
    "test_size      = 0.2"
   ],
   "id": "7efc89d737e00b82",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T13:25:48.437340Z",
     "start_time": "2025-05-21T13:25:43.671242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 0. Configuration\n",
    "# -----------------------------\n",
    "device         = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_dir       = '../raw'                   # image data path\n",
    "text_data_path = '../Sample Data Texts.xlsx' # text-data Excel\n",
    "image_size     = (32, 32)\n",
    "batch_size     = 32\n",
    "lr_img         = lr\n",
    "# -----------------------------\n",
    "# 1. Image loading & preprocessing\n",
    "# -----------------------------\n",
    "images, labels = [], []\n",
    "for cls in os.listdir(data_dir):\n",
    "    cls_path = os.path.join(data_dir, cls)\n",
    "    if not os.path.isdir(cls_path):\n",
    "        continue\n",
    "    for fn in os.listdir(cls_path):\n",
    "        if fn.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "            try:\n",
    "                img = Image.open(os.path.join(cls_path, fn)).convert('L')\n",
    "                img = img.resize(image_size)\n",
    "                arr = np.array(img, dtype=np.float32) / 255.0\n",
    "                images.append(arr)\n",
    "                labels.append(cls)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {fn}: {e}\")\n",
    "\n",
    "images = np.stack(images)  # (N, H, W)\n",
    "le     = LabelEncoder()\n",
    "y      = le.fit_transform(labels)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, imgs, lbls):\n",
    "        self.imgs = imgs\n",
    "        self.lbls = lbls\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.expand_dims(self.imgs[idx], 0)\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(self.lbls[idx], dtype=torch.long)\n",
    "\n",
    "idx = np.arange(len(images))\n",
    "tidx, vidx = train_test_split(idx,test_size=test_size, random_state=42, stratify=y)\n",
    "train_ds = ImageDataset(images[tidx], y[tidx])\n",
    "test_ds  = ImageDataset(images[vidx], y[vidx])\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Text prior extraction + BERT fine-tuning\n",
    "# -----------------------------\n",
    "df = pd.read_excel(text_data_path)\n",
    "combined_text = df['Type'].iloc[0] + ' ' + df['List of Store Names'].iloc[0]\n",
    "\n",
    "tokenizer  = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-chinese\").to(device)\n",
    "\n",
    "# Freeze all layers\n",
    "for p in bert_model.parameters():\n",
    "    p.requires_grad = False\n",
    "# Unfreeze the last two encoder layers\n",
    "for name, p in bert_model.named_parameters():\n",
    "    if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n",
    "        p.requires_grad = True\n",
    "bert_model.train()\n",
    "\n",
    "# Pre-compute a fixed text vector\n",
    "with torch.no_grad():\n",
    "    txt_inputs = tokenizer(combined_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    txt_inputs = {k: v.to(device) for k, v in txt_inputs.items()}\n",
    "    txt_vec = bert_model(**txt_inputs).last_hidden_state[:, 0, :].squeeze(0).to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Define the ResNet + text-fusion model\n",
    "# -----------------------------\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, 3, stride, 1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(planes)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, 1, 1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(planes)\n",
    "        self.down  = None\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.down = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        idt = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.down:\n",
    "            idt = self.down(x)\n",
    "        return self.relu(out + idt)\n",
    "\n",
    "class ResNetFusionText(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes, in_ch=1, text_dim=768):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(in_ch, 64, 3, 1, 1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(64)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        def mk(planes, cnt, stride):\n",
    "            seq = [block(self.in_planes, planes, stride)]\n",
    "            self.in_planes = planes * block.expansion\n",
    "            for _ in range(1, cnt):\n",
    "                seq.append(block(self.in_planes, planes))\n",
    "            return nn.Sequential(*seq)\n",
    "        self.layer1 = mk(64,  layers[0], 1)\n",
    "        self.layer2 = mk(128, layers[1], 2)\n",
    "        self.layer3 = mk(256, layers[2], 2)\n",
    "        self.layer4 = mk(512, layers[3], 2)\n",
    "        self.avgp   = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.w_img  = nn.Parameter(torch.tensor(1.0, device=device))\n",
    "        self.w_text = nn.Parameter(torch.tensor(1.0, device=device))\n",
    "        self.fc     = nn.Linear(512 + text_dim, num_classes)\n",
    "    def forward(self, x, text_vec):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x); x = self.layer2(x)\n",
    "        x = self.layer3(x); x = self.layer4(x)\n",
    "        x = self.avgp(x).flatten(1)\n",
    "        B = x.size(0)\n",
    "        img_f = x * self.w_img\n",
    "        txt_f = text_vec.unsqueeze(0).expand(B, -1) * self.w_text\n",
    "        return self.fc(torch.cat([img_f, txt_f], dim=1))\n",
    "\n",
    "def build_model(n_cls):\n",
    "    return ResNetFusionText(BasicBlock, [2, 2, 2, 2], n_cls).to(device)\n",
    "\n",
    "model = build_model(num_classes)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Optimizer & LR scheduler\n",
    "# -----------------------------\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': model.parameters(),                       'lr': lr_img},\n",
    "    {'params': bert_model.encoder.layer[10].parameters(),'lr': lr_bert},\n",
    "    {'params': bert_model.encoder.layer[11].parameters(),'lr': lr_bert},\n",
    "], weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Training\n",
    "# -----------------------------\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    bert_model.train()\n",
    "    total_loss = 0.0\n",
    "    for imgs, lbls in train_loader:\n",
    "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "        inputs = tokenizer(combined_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        batch_txt_vec = bert_model(**inputs).last_hidden_state[:, 0, :].squeeze(0)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(imgs, batch_txt_vec), lbls)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} â€” loss: {total_loss/len(train_ds):.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Testing & metrics + visualization\n",
    "# -----------------------------\n",
    "model.eval()\n",
    "bert_model.eval()\n",
    "all_true, all_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, lbls in test_loader:\n",
    "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "        inputs = tokenizer(combined_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        batch_txt_vec = bert_model(**inputs).last_hidden_state[:, 0, :].squeeze(0)\n",
    "        preds = model(imgs, batch_txt_vec).argmax(dim=1)\n",
    "        all_true.extend(lbls.cpu().tolist())\n",
    "        all_pred.extend(preds.cpu().tolist())\n",
    "\n",
    "# Accuracy\n",
    "print(f\"Test Accuracy: {(np.array(all_pred) == np.array(all_true)).mean():.4f}\")\n",
    "# Other metrics\n",
    "print(f\"Precision (macro): {precision_score(all_true, all_pred, average='macro'):.4f}\")\n",
    "print(f\"Recall    (macro): {recall_score(all_true, all_pred, average='macro'):.4f}\")\n",
    "print(f\"F1-score  (macro): {f1_score(all_true, all_pred, average='macro'):.4f}\")\n",
    "\n"
   ],
   "id": "26393b37260a4c9a",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 35\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fn\u001B[38;5;241m.\u001B[39mlower()\u001B[38;5;241m.\u001B[39mendswith((\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.jpg\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.jpeg\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.png\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.bmp\u001B[39m\u001B[38;5;124m'\u001B[39m)):\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 35\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcls_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mL\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     36\u001B[0m         img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mresize(image_size)\n\u001B[0;32m     37\u001B[0m         arr \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(img, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m255.0\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\torch_gpu\\lib\\site-packages\\PIL\\Image.py:984\u001B[0m, in \u001B[0;36mImage.convert\u001B[1;34m(self, mode, matrix, dither, palette, colors)\u001B[0m\n\u001B[0;32m    981\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBGR;15\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBGR;16\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBGR;24\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    982\u001B[0m     deprecate(mode, \u001B[38;5;241m12\u001B[39m)\n\u001B[1;32m--> 984\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    986\u001B[0m has_transparency \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransparency\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\n\u001B[0;32m    987\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mP\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    988\u001B[0m     \u001B[38;5;66;03m# determine default mode\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\torch_gpu\\lib\\site-packages\\PIL\\ImageFile.py:300\u001B[0m, in \u001B[0;36mImageFile.load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    297\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(msg)\n\u001B[0;32m    299\u001B[0m b \u001B[38;5;241m=\u001B[39m b \u001B[38;5;241m+\u001B[39m s\n\u001B[1;32m--> 300\u001B[0m n, err_code \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    302\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T17:57:09.215746Z",
     "start_time": "2025-05-05T17:57:09.201593Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7cb98bbb298f3f74",
   "outputs": [],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
