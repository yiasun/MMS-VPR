{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_size=0.2\n",
    "in_channels=1\n",
    "embed_dim=64\n",
    "num_layers=6\n",
    "num_heads=8 \n",
    "mlp_dim=128\n",
    "dropout=0.1\n",
    "text_dim=768\n",
    "lr=0.001\n",
    "gamma=0.1\n",
    "num_epochs = 100"
   ],
   "id": "3483cbdd5fb7bbd0"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Image Data Loading and Preprocessing\n",
    "# -------------------------------\n",
    "data_dir = '../raw'  # Change to your image dataset folder path\n",
    "\n",
    "images = []  # Store image data, keep 2D shape (32, 32) for patch splitting\n",
    "labels = []  # Store class labels\n",
    "\n",
    "for class_name in os.listdir(data_dir):\n",
    "    class_path = os.path.join(data_dir, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        for filename in os.listdir(class_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "                file_path = os.path.join(class_path, filename)\n",
    "                try:\n",
    "                    img = Image.open(file_path)\n",
    "                    img = img.convert('L')            # Convert to grayscale\n",
    "                    img = img.resize((32, 32))        # Resize to 32Ã—32\n",
    "                    img_array = np.array(img)         # Keep 2D (32, 32)\n",
    "                    images.append(img_array)\n",
    "                    labels.append(class_name)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "# Normalize to [0,1], shape: (N, 32, 32)\n",
    "images = np.array(images, dtype=np.float32) / 255.0\n",
    "labels = np.array(labels)\n",
    "print(\"Total number of images read:\", images.shape[0])\n",
    "print(\"Image dimensions:\", images.shape[1:])\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(labels)\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Number of encoded classes:\", num_classes)\n",
    "\n",
    "# Convert images to (N, C, H, W) format, here C=1\n",
    "images = np.expand_dims(images, axis=1)  # Shape: (N, 1, 32, 32)\n",
    "\n",
    "# Split into training and test sets\n",
    "indices = np.arange(len(images))\n",
    "train_idx, test_idx = train_test_split(\n",
    "    indices, test_size=test_size, random_state=42, stratify=labels_encoded\n",
    ")\n",
    "\n",
    "x_tensor = torch.tensor(images, dtype=torch.float)   # (N, 1, 32, 32)\n",
    "y_tensor = torch.tensor(labels_encoded, dtype=torch.long)\n",
    "\n",
    "# Split data by index\n",
    "x_train = x_tensor[train_idx]\n",
    "y_train = y_tensor[train_idx]\n",
    "x_test  = x_tensor[test_idx]\n",
    "y_test  = y_tensor[test_idx]\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "test_dataset  = TensorDataset(x_test, y_test)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader   = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Text Prior: Extract text embedding using BERT\n",
    "# -------------------------------\n",
    "# Change text_data_path to your Excel file path\n",
    "text_data_path = '../Sample Data Texts.xlsx'\n",
    "df_text = pd.read_excel(text_data_path)\n",
    "# Use the first row of \"List of Store Names\" as plugin information\n",
    "first_text = df_text['List of Store Names'].iloc[0]\n",
    "print(\"First row text:\", first_text)\n",
    "\n",
    "# Use pre-trained BERT model (e.g., bert-base-chinese if text is Chinese)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-chinese\")\n",
    "bert_model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(first_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = bert_model(**inputs)\n",
    "    # Take the [CLS] token's hidden state, shape: (1, 768)\n",
    "    text_hidden_state = outputs.last_hidden_state[:, 0, :]\n",
    "    text_hidden_state = text_hidden_state.squeeze(0)  # (768,)\n",
    "print(\"Text hidden state shape:\", text_hidden_state.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Define ViT Model (Plugin-based Multimodal ViT)\n",
    "# -------------------------------\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Split input image into patches and apply convolution for linear projection to obtain embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=1, embed_dim=64):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        x = self.proj(x)              # (B, embed_dim, H/patch_size, W/patch_size)\n",
    "        x = x.flatten(2)              # (B, embed_dim, num_patches)\n",
    "        x = x.transpose(1, 2)         # (B, num_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "class ViTPlugin(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=1, embed_dim=64,\n",
    "                 num_layers=6, num_heads=8, mlp_dim=128, num_classes=10,\n",
    "                 dropout=0.1, text_dim=768):\n",
    "        super(ViTPlugin, self).__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        # Classification token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        # Positional encoding (including classification token)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads,\n",
    "            dim_feedforward=mlp_dim, dropout=dropout\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        # Combined dimension after fusion: embed_dim + text_dim\n",
    "        self.head = nn.Linear(embed_dim + text_dim, num_classes)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.xavier_uniform_(self.head.weight)\n",
    "        nn.init.zeros_(self.head.bias)\n",
    "    \n",
    "    def forward(self, x, text_vector):\n",
    "        # x: (B, C, H, W)\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)                    # (B, num_patches, embed_dim)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, embed_dim)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)          # (B, num_patches+1, embed_dim)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        # Transformer expects input shape: (sequence_length, batch_size, embed_dim)\n",
    "        x = x.transpose(0, 1)                      # (num_patches+1, B, embed_dim)\n",
    "        x = self.transformer(x)                    # (num_patches+1, B, embed_dim)\n",
    "        x = x.transpose(0, 1)                      # (B, num_patches+1, embed_dim)\n",
    "        x = self.norm(x)\n",
    "        cls_out = x[:, 0]                          # Classification token output, (B, embed_dim)\n",
    "        # Expand fixed text vector (text_dim,) to (B, text_dim)\n",
    "        text_expanded = text_vector.unsqueeze(0).expand(B, -1)\n",
    "        # Fuse: concatenate classification token output and text embedding\n",
    "        fused = torch.cat([cls_out, text_expanded], dim=1)  # (B, embed_dim + text_dim)\n",
    "        logits = self.head(fused)\n",
    "        return logits\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Training and Evaluation\n",
    "# -------------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "text_hidden_state = text_hidden_state.to(device)\n",
    "\n",
    "model = ViTPlugin(\n",
    "    img_size=32, patch_size=4, in_channels=in_channels, embed_dim=embed_dim,\n",
    "    num_layers=num_layers, num_heads=num_heads, mlp_dim=mlp_dim, num_classes=num_classes,\n",
    "    dropout=dropout, text_dim=text_dim\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=gamma)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch_images, batch_labels in train_loader:\n",
    "        batch_images = batch_images.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_images, text_hidden_state)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * batch_images.size(0)\n",
    "    \n",
    "    scheduler.step()\n",
    "    avg_loss = epoch_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Evaluate every 20 epochs\n",
    "    if epoch % 20 == 0:\n",
    "        model.eval()\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls in test_loader:\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                preds = model(imgs, text_hidden_state).argmax(dim=1)\n",
    "                correct += (preds == lbls).sum().item()\n",
    "                total += lbls.size(0)\n",
    "        acc = correct / total\n",
    "        print(f\"Epoch {epoch:03d}, Loss: {avg_loss:.4f}, Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "model.eval()\n",
    "correct = total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, lbls in test_loader:\n",
    "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "        preds = model(imgs, text_hidden_state).argmax(dim=1)\n",
    "        correct += (preds == lbls).sum().item()\n",
    "        total += lbls.size(0)\n",
    "acc = correct / total\n",
    "print(f\"Final Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Additional: compute Precision / Recall / F1\n",
    "# -----------------------------\n",
    "y_true, y_pred = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs, lbls in test_loader:\n",
    "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "        preds = model(imgs, text_hidden_state).argmax(dim=1)\n",
    "        y_true.extend(lbls.cpu().tolist())\n",
    "        y_pred.extend(preds.cpu().tolist())\n",
    "\n",
    "prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "rec  = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "f1   = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "print(f\"Final Test Set â€” Precision: {prec:.4f} | Recall: {rec:.4f} | F1-score: {f1:.4f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "babf5385cf848c0d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
